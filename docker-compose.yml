version: '3.8'

services:
  # ============================================
  # TypeDB - Knowledge Graph Database
  # ============================================
  typedb:
    image: vaticle/typedb:3.0.0
    container_name: superhyperion-typedb
    ports:
      - "1729:1729"  # TypeDB gRPC port
    volumes:
      - typedb-data:/opt/typedb-all-linux-x86_64/server/data
    environment:
      - TYPEDB_OPTS=--diagnostics.monitoring.enable=true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:1729"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # ============================================
  # Ollama - Local LLM Inference
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: superhyperion-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # ============================================
  # Jupyter - CodeAct Sandbox
  # ============================================
  jupyter:
    image: jupyter/minimal-notebook:python-3.12
    container_name: superhyperion-jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=superhyperion
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./src:/home/jovyan/src:ro
    restart: unless-stopped

  # ============================================
  # FastAPI Backend (dev mode)
  # ============================================
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: superhyperion-api
    ports:
      - "8000:8000"
    environment:
      - TYPEDB_HOST=typedb
      - TYPEDB_PORT=1729
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
    volumes:
      - ./src:/app/src
    depends_on:
      - typedb
      - ollama
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  # ============================================
  # Streamlit Frontend (dev mode)
  # ============================================
  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.ui
    container_name: superhyperion-ui
    ports:
      - "8501:8501"
    environment:
      - API_HOST=api
      - API_PORT=8000
    volumes:
      - ./src:/app/src
    depends_on:
      - api
    command: streamlit run src/ui/app.py --server.address 0.0.0.0
    restart: unless-stopped

volumes:
  typedb-data:
    name: superhyperion-typedb-data
  ollama-models:
    name: superhyperion-ollama-models

networks:
  default:
    name: superhyperion-network
